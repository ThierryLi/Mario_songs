{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Musics retrieved from http://www.midishrine.com/index.php?console=sns\n",
    "\n",
    "https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy\n",
    "from music21 import converter, instrument, note, chord\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network():\n",
    "    \"\"\" Train a Neural Network to generate music \"\"\"\n",
    "    notes = get_notes()\n",
    "\n",
    "    # get amount of pitch names\n",
    "    n_vocab = len(set(notes))\n",
    "\n",
    "    network_input, network_output = prepare_sequences(notes, n_vocab)\n",
    "\n",
    "    model = create_network(network_input, n_vocab)\n",
    "\n",
    "    train(model, network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_notes():\n",
    "    \"\"\" Get all the notes and chords from the midi files in the ./midi_songs directory \"\"\"\n",
    "    notes = []\n",
    "\n",
    "    for file in glob.glob(\"input/*.mid\"):\n",
    "        display(file)\n",
    "        midi = converter.parse(file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        parts = instrument.partitionByInstrument(midi)\n",
    "\n",
    "        if parts: # file has instrument parts\n",
    "            notes_to_parse = parts.parts[0].recurse()\n",
    "        else: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    with open('data/notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 100\n",
    "\n",
    "    # get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "     # create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    network_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "\n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_network(network_input, n_vocab):\n",
    "    \"\"\" create the structure of the neural network \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        512,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(512, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(512))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, network_input, network_output):\n",
    "    \"\"\" train the neural network \"\"\"\n",
    "    filepath = \"04_05weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath,\n",
    "        monitor='loss',\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        mode='min'\n",
    "    )\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    model.fit(network_input, network_output, epochs=200, batch_size=64, callbacks=callbacks_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input/smb1-1-1_Remix2.mid'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'input/smb1-1-2_Remix2.mid'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'input/smb1-Castle.mid'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'input/smb1-star.mid'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'input/smb1-1-2_Remix.mid'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'input/smb1-1-1_Remix.mid'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'input/smb1-1-1_Remix3.mid'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'input/smb1-1-1_Remix4.mid'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'input/smb1-Theme.mid'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 100, 512)          1052672   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 100, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 77)                19789     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 77)                0         \n",
      "=================================================================\n",
      "Total params: 5,402,189\n",
      "Trainable params: 5,402,189\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "10564/10564 [==============================] - 59s 6ms/step - loss: 3.7996\n",
      "Epoch 2/200\n",
      "10564/10564 [==============================] - 59s 6ms/step - loss: 3.6493\n",
      "Epoch 3/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6430\n",
      "Epoch 4/200\n",
      "10564/10564 [==============================] - 61s 6ms/step - loss: 3.6344\n",
      "Epoch 5/200\n",
      "10564/10564 [==============================] - 61s 6ms/step - loss: 3.6298\n",
      "Epoch 6/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6244\n",
      "Epoch 7/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6254\n",
      "Epoch 8/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6242\n",
      "Epoch 9/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6219\n",
      "Epoch 10/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6210\n",
      "Epoch 11/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6202\n",
      "Epoch 12/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6188\n",
      "Epoch 13/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6163\n",
      "Epoch 14/200\n",
      "10564/10564 [==============================] - 64s 6ms/step - loss: 3.6173\n",
      "Epoch 15/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6169\n",
      "Epoch 16/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6177\n",
      "Epoch 17/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6164\n",
      "Epoch 18/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6166\n",
      "Epoch 19/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6166\n",
      "Epoch 20/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6163\n",
      "Epoch 21/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6173\n",
      "Epoch 22/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6173\n",
      "Epoch 23/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6167\n",
      "Epoch 24/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6176\n",
      "Epoch 25/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6155\n",
      "Epoch 26/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6157\n",
      "Epoch 27/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6177\n",
      "Epoch 28/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6154\n",
      "Epoch 29/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6149\n",
      "Epoch 30/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6162\n",
      "Epoch 31/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6167\n",
      "Epoch 32/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6164\n",
      "Epoch 33/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6157\n",
      "Epoch 34/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6160\n",
      "Epoch 35/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6169\n",
      "Epoch 36/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6162\n",
      "Epoch 37/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6155\n",
      "Epoch 38/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6157\n",
      "Epoch 39/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6170\n",
      "Epoch 40/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6171\n",
      "Epoch 41/200\n",
      "10564/10564 [==============================] - 64s 6ms/step - loss: 3.6152\n",
      "Epoch 42/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6159\n",
      "Epoch 43/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6154\n",
      "Epoch 44/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6160\n",
      "Epoch 45/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6161\n",
      "Epoch 46/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6146\n",
      "Epoch 47/200\n",
      "10564/10564 [==============================] - 61s 6ms/step - loss: 3.6159\n",
      "Epoch 48/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6175\n",
      "Epoch 49/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6175\n",
      "Epoch 50/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6164\n",
      "Epoch 51/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6160\n",
      "Epoch 52/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6174\n",
      "Epoch 53/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6157\n",
      "Epoch 54/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6156\n",
      "Epoch 55/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6167\n",
      "Epoch 56/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6160\n",
      "Epoch 57/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6150\n",
      "Epoch 58/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6172\n",
      "Epoch 59/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6161\n",
      "Epoch 60/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6169\n",
      "Epoch 61/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6166\n",
      "Epoch 62/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6154\n",
      "Epoch 63/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6165\n",
      "Epoch 64/200\n",
      "10564/10564 [==============================] - 64s 6ms/step - loss: 3.6176\n",
      "Epoch 65/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6165\n",
      "Epoch 66/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6159\n",
      "Epoch 67/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6154\n",
      "Epoch 68/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6170\n",
      "Epoch 69/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6152\n",
      "Epoch 70/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6174\n",
      "Epoch 71/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6150\n",
      "Epoch 72/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6156\n",
      "Epoch 73/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6162\n",
      "Epoch 74/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6165\n",
      "Epoch 75/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6157\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6168\n",
      "Epoch 77/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6154\n",
      "Epoch 78/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6163\n",
      "Epoch 79/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6160\n",
      "Epoch 80/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6163\n",
      "Epoch 81/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6152\n",
      "Epoch 82/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6165\n",
      "Epoch 83/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6159\n",
      "Epoch 84/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6163\n",
      "Epoch 85/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6164\n",
      "Epoch 86/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6163\n",
      "Epoch 87/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6157\n",
      "Epoch 88/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6163\n",
      "Epoch 89/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6164\n",
      "Epoch 90/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6165\n",
      "Epoch 91/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6153\n",
      "Epoch 92/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6152\n",
      "Epoch 93/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6159\n",
      "Epoch 94/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6154\n",
      "Epoch 95/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6160\n",
      "Epoch 96/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6158\n",
      "Epoch 97/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6143\n",
      "Epoch 98/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6156\n",
      "Epoch 99/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6162\n",
      "Epoch 100/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6153\n",
      "Epoch 101/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6164\n",
      "Epoch 102/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6158\n",
      "Epoch 103/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6165\n",
      "Epoch 104/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6149\n",
      "Epoch 105/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6153\n",
      "Epoch 106/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6146\n",
      "Epoch 107/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6158\n",
      "Epoch 108/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6191\n",
      "Epoch 109/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6166\n",
      "Epoch 110/200\n",
      "10564/10564 [==============================] - 64s 6ms/step - loss: 3.6159\n",
      "Epoch 111/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6181\n",
      "Epoch 112/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6167\n",
      "Epoch 113/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6154\n",
      "Epoch 114/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6164\n",
      "Epoch 115/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6159\n",
      "Epoch 116/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6159\n",
      "Epoch 117/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6157\n",
      "Epoch 118/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6169\n",
      "Epoch 119/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6167\n",
      "Epoch 120/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6153\n",
      "Epoch 121/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6165\n",
      "Epoch 122/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6161\n",
      "Epoch 123/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6162\n",
      "Epoch 124/200\n",
      "10564/10564 [==============================] - 64s 6ms/step - loss: 3.6158\n",
      "Epoch 125/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6167\n",
      "Epoch 126/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6151\n",
      "Epoch 127/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6152\n",
      "Epoch 128/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6164\n",
      "Epoch 129/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6158\n",
      "Epoch 130/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6151\n",
      "Epoch 131/200\n",
      "10564/10564 [==============================] - 64s 6ms/step - loss: 3.6147\n",
      "Epoch 132/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6147\n",
      "Epoch 133/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6141\n",
      "Epoch 134/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6153\n",
      "Epoch 135/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6145\n",
      "Epoch 136/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6144\n",
      "Epoch 137/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6147\n",
      "Epoch 138/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6148\n",
      "Epoch 139/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6165\n",
      "Epoch 140/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6157\n",
      "Epoch 141/200\n",
      "10564/10564 [==============================] - 64s 6ms/step - loss: 3.6162\n",
      "Epoch 142/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6152\n",
      "Epoch 143/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6154\n",
      "Epoch 144/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6165\n",
      "Epoch 145/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6153\n",
      "Epoch 146/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6152\n",
      "Epoch 147/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6158\n",
      "Epoch 148/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6147\n",
      "Epoch 149/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6148\n",
      "Epoch 150/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6151\n",
      "Epoch 151/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6149\n",
      "Epoch 152/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6156\n",
      "Epoch 153/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6159\n",
      "Epoch 154/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6152\n",
      "Epoch 155/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6153\n",
      "Epoch 156/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6151\n",
      "Epoch 157/200\n",
      "10564/10564 [==============================] - 64s 6ms/step - loss: 3.6159\n",
      "Epoch 158/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6156\n",
      "Epoch 159/200\n",
      "10564/10564 [==============================] - 64s 6ms/step - loss: 3.6157\n",
      "Epoch 160/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6154\n",
      "Epoch 161/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6149\n",
      "Epoch 162/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6144\n",
      "Epoch 163/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6156\n",
      "Epoch 164/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6151\n",
      "Epoch 165/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6161\n",
      "Epoch 166/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6145\n",
      "Epoch 167/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6151\n",
      "Epoch 168/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6150\n",
      "Epoch 169/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6153\n",
      "Epoch 170/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6153\n",
      "Epoch 171/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6150\n",
      "Epoch 172/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6146\n",
      "Epoch 173/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6148\n",
      "Epoch 174/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6148\n",
      "Epoch 175/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6155\n",
      "Epoch 176/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6154\n",
      "Epoch 177/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6140\n",
      "Epoch 178/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6153\n",
      "Epoch 179/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6144\n",
      "Epoch 180/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6138\n",
      "Epoch 181/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6148\n",
      "Epoch 182/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6159\n",
      "Epoch 183/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6149\n",
      "Epoch 184/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6146\n",
      "Epoch 185/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6155\n",
      "Epoch 186/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6150\n",
      "Epoch 187/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6141\n",
      "Epoch 188/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6152\n",
      "Epoch 189/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6149\n",
      "Epoch 190/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6154\n",
      "Epoch 191/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6151\n",
      "Epoch 192/200\n",
      "10564/10564 [==============================] - 63s 6ms/step - loss: 3.6156\n",
      "Epoch 193/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6148\n",
      "Epoch 194/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6150\n",
      "Epoch 195/200\n",
      "10564/10564 [==============================] - 64s 6ms/step - loss: 3.6145\n",
      "Epoch 196/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6155\n",
      "Epoch 197/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6147\n",
      "Epoch 198/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6142\n",
      "Epoch 199/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6157\n",
      "Epoch 200/200\n",
      "10564/10564 [==============================] - 62s 6ms/step - loss: 3.6154\n"
     ]
    }
   ],
   "source": [
    "train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9a7881f870d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "from music21 import instrument, note, stream, chord\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate():\n",
    "    \"\"\" Generate a piano midi file \"\"\"\n",
    "    #load the notes used to train the model\n",
    "    with open('data/notes', 'rb') as filepath:\n",
    "        notes = pickle.load(filepath)\n",
    "\n",
    "    # Get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    # Get all pitch names\n",
    "    n_vocab = len(set(notes))\n",
    "\n",
    "    network_input, normalized_input = prepare_sequences(notes, pitchnames, n_vocab)\n",
    "    model = create_network(normalized_input, n_vocab)\n",
    "    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
    "    create_midi(prediction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, pitchnames, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    # map between notes and integers and back\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    sequence_length = 100\n",
    "    network_input = []\n",
    "    output = []\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    normalized_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    normalized_input = normalized_input / float(n_vocab)\n",
    "\n",
    "    return (network_input, normalized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_network(network_input, n_vocab):\n",
    "    \"\"\" create the structure of the neural network \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        512,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(512, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(512))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    # Load the weights to each node\n",
    "    model.load_weights('./best-weights-without-rests.hdf5')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # pick a random sequence from the input as a starting point for the prediction\n",
    "    start = numpy.random.randint(0, len(network_input)-1)\n",
    "\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    pattern = network_input[start]\n",
    "    prediction_output = []\n",
    "\n",
    "    # generate 500 notes\n",
    "    for note_index in range(500):\n",
    "        prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "\n",
    "        index = numpy.argmax(prediction)\n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_midi(prediction_output):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "    midi_stream.write('midi', fp='04_05_test_output.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
